{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b0e3af",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-08-01T12:20:51.690972Z",
     "iopub.status.busy": "2021-08-01T12:20:51.689491Z",
     "iopub.status.idle": "2021-08-01T12:21:26.819105Z",
     "shell.execute_reply": "2021-08-01T12:21:26.817972Z",
     "shell.execute_reply.started": "2021-08-01T05:40:18.073208Z"
    },
    "papermill": {
     "duration": 35.144094,
     "end_time": "2021-08-01T12:21:26.819290",
     "exception": false,
     "start_time": "2021-08-01T12:20:51.675196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.19.5)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (0.23.2)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (4.61.1)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.6.3)\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.7.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.0.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (2.1.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.18.2)\r\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (3.7.4.3)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.6)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-3.1.1\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Processing /kaggle/input/iterative-stratification/iterative-stratification-master\r\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\r\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.19.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.6.3)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (0.23.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (1.0.1)\r\n",
      "Building wheels for collected packages: iterative-stratification\r\n",
      "  Building wheel for iterative-stratification (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for iterative-stratification: filename=iterative_stratification-0.1.6-py3-none-any.whl size=8401 sha256=4fdf11b1fcfc3cf20a713514c205c9b6928ffde8b4ac5c0dad39357e8a8f404c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/47/3f/eb4af42d124f37d23d6f13a4c8bbc32c1d70140e6e1cecb4aa\r\n",
      "Successfully built iterative-stratification\r\n",
      "Installing collected packages: iterative-stratification\r\n",
      "Successfully installed iterative-stratification-0.1.6\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl pytorch-tabnet\n",
    "!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8469147d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T12:21:26.853027Z",
     "iopub.status.busy": "2021-08-01T12:21:26.852267Z",
     "iopub.status.idle": "2021-08-01T12:21:26.873108Z",
     "shell.execute_reply": "2021-08-01T12:21:26.872680Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.347265Z"
    },
    "papermill": {
     "duration": 0.039744,
     "end_time": "2021-08-01T12:21:26.873229",
     "exception": false,
     "start_time": "2021-08-01T12:21:26.833485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/lish-moa/train_targets_scored.csv\n",
      "/kaggle/input/lish-moa/sample_submission.csv\n",
      "/kaggle/input/lish-moa/train_drug.csv\n",
      "/kaggle/input/lish-moa/train_targets_nonscored.csv\n",
      "/kaggle/input/lish-moa/train_features.csv\n",
      "/kaggle/input/lish-moa/test_features.csv\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.0-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.1-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-1.2.0-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-3.0.0-py3-none-any.whl\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/.travis.yml\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/setup.cfg\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/LICENSE\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/.gitignore\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/README.md\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/setup.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/tests/test_ml_stratifiers.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/tests/__init__.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/iterstrat/ml_stratifiers.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/iterstrat/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f96e23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T12:21:26.910615Z",
     "iopub.status.busy": "2021-08-01T12:21:26.909713Z",
     "iopub.status.idle": "2021-08-01T12:21:29.149194Z",
     "shell.execute_reply": "2021-08-01T12:21:29.148695Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.373148Z"
    },
    "papermill": {
     "duration": 2.261371,
     "end_time": "2021-08-01T12:21:29.149326",
     "exception": false,
     "start_time": "2021-08-01T12:21:26.887955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import log_loss\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0416e3f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T12:21:29.191958Z",
     "iopub.status.busy": "2021-08-01T12:21:29.191169Z",
     "iopub.status.idle": "2021-08-01T12:21:29.195124Z",
     "shell.execute_reply": "2021-08-01T12:21:29.194597Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.382224Z"
    },
    "papermill": {
     "duration": 0.031573,
     "end_time": "2021-08-01T12:21:29.195251",
     "exception": false,
     "start_time": "2021-08-01T12:21:29.163678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features.values\n",
    "        self.targets = targets.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx_features = torch.tensor(self.features[idx], dtype=torch.float)\n",
    "        idx_targets = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return idx_features, idx_targets\n",
    "\n",
    "\n",
    "class TestMoADataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx_features = torch.tensor(self.features[idx], dtype=torch.float)\n",
    "        return idx_features\n",
    "    \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,\n",
    "                 output_size, dropout=0.2):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b13bea8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T12:21:29.226839Z",
     "iopub.status.busy": "2021-08-01T12:21:29.226104Z",
     "iopub.status.idle": "2021-08-01T12:21:29.229303Z",
     "shell.execute_reply": "2021-08-01T12:21:29.229764Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.395437Z"
    },
    "papermill": {
     "duration": 0.020698,
     "end_time": "2021-08-01T12:21:29.229908",
     "exception": false,
     "start_time": "2021-08-01T12:21:29.209210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogitsLogLoss(Metric):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = 'logits_ll'\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        aux = (1 - y_true) * np.log(1 - expit(y_pred) + 1e-15) + y_true * np.log(expit(y_pred) + 1e-15)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3623bd7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T12:21:29.260716Z",
     "iopub.status.busy": "2021-08-01T12:21:29.260038Z",
     "iopub.status.idle": "2021-08-01T12:21:29.263595Z",
     "shell.execute_reply": "2021-08-01T12:21:29.263195Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.411053Z"
    },
    "papermill": {
     "duration": 0.020522,
     "end_time": "2021-08-01T12:21:29.263706",
     "exception": false,
     "start_time": "2021-08-01T12:21:29.243184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed, use_cuda=False):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f516f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T12:21:29.301444Z",
     "iopub.status.busy": "2021-08-01T12:21:29.300831Z",
     "iopub.status.idle": "2021-08-01T12:21:29.305065Z",
     "shell.execute_reply": "2021-08-01T12:21:29.304586Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.420247Z"
    },
    "papermill": {
     "duration": 0.027472,
     "end_time": "2021-08-01T12:21:29.305185",
     "exception": false,
     "start_time": "2021-08-01T12:21:29.277713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    df['cp_time'] = df['cp_time'].map({24: 0, 48: 1, 72:2})\n",
    "    df = df[df['cp_type']  != 'ctl_vehicle'].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_data(dir):\n",
    "    X_train = pd.read_csv(f'{dir}/train_features.csv')\n",
    "    Y_train = pd.read_csv(f'{dir}/train_targets_scored.csv')\n",
    "    X_test = pd.read_csv(f'{dir}/test_features.csv')\n",
    "    ss = pd.read_csv(f'{dir}/sample_submission.csv')\n",
    "\n",
    "    train = X_train.merge(Y_train, on='sig_id')\n",
    "    Y_train_stub = train.loc[:, Y_train.columns]\n",
    "    \n",
    "    train = preprocess(train)\n",
    "    X_test = preprocess(X_test).drop(['cp_type'], axis=1)\n",
    "\n",
    "    X_train = train.loc[:, X_train.columns].drop(['sig_id', 'cp_type'], axis=1)\n",
    "    Y_train = train.loc[:, Y_train.columns]\n",
    "\n",
    "    return X_train, Y_train, Y_train_stub, X_test, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a120ccf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T12:21:29.335743Z",
     "iopub.status.busy": "2021-08-01T12:21:29.335211Z",
     "iopub.status.idle": "2021-08-01T12:21:29.338665Z",
     "shell.execute_reply": "2021-08-01T12:21:29.339134Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.43316Z"
    },
    "papermill": {
     "duration": 0.020102,
     "end_time": "2021-08-01T12:21:29.339265",
     "exception": false,
     "start_time": "2021-08-01T12:21:29.319163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TAB_EPOCHS = 200\n",
    "TAB_BATCH_SIZE = 1024\n",
    "TAB_LEARNING_RATE = 2e-2\n",
    "WEIGHT_DECAY = 1e-5\n",
    "SEED = 42\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 2048\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c01eec61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T12:21:29.498836Z",
     "iopub.status.busy": "2021-08-01T12:21:29.497752Z",
     "iopub.status.idle": "2021-08-01T12:21:29.500779Z",
     "shell.execute_reply": "2021-08-01T12:21:29.500280Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.443431Z"
    },
    "papermill": {
     "duration": 0.148314,
     "end_time": "2021-08-01T12:21:29.500948",
     "exception": false,
     "start_time": "2021-08-01T12:21:29.352634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fun(model, optimizer, loss_fun, train_loader, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fun(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 5 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            print(f'Train Epoch: {epoch + 1}, Batch: [{(batch_idx + 1)}/{len(train_loader)}], Loss: {loss.item():.3f}')\n",
    "\n",
    "    mean_loss = running_loss / len(train_loader)\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def validate_fun(model, loss_fun, val_loader, device, epoch):\n",
    "    Y_pred_lst = []\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        loss = loss_fun(outputs, targets)\n",
    "        running_loss += loss.item()\n",
    "        Y_pred_lst.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "        if (batch_idx + 1) % 1024 == 0 or (batch_idx + 1) == len(val_loader):\n",
    "            print(f'Validate Epoch: {epoch + 1}, Batch: [{batch_idx + 1}/{len(val_loader)}], Loss: {loss.item():.6f}')\n",
    "\n",
    "    mean_loss = running_loss / len(val_loader)\n",
    "    Y_pred = np.concatenate(Y_pred_lst)\n",
    "\n",
    "    return mean_loss, Y_pred\n",
    "\n",
    "\n",
    "def test_fun(model, test_loader, device):\n",
    "    Y_pred_lst = []\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, inputs in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        Y_pred_lst.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "    Y_pred = np.concatenate(Y_pred_lst)\n",
    "\n",
    "    return Y_pred\n",
    "\n",
    "\n",
    "def train_simple_net(fold, X_test, train_loader, val_loader, train_size, val_idx, in_size, out_size,\n",
    "                     device):\n",
    "    model = SimpleNet(in_size, 2048, 1024, out_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    loss_fun = nn.BCEWithLogitsLoss()\n",
    "    best_loss = np.inf\n",
    "    oof = np.zeros((train_size, out_size))\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_train_loss = train_fun(model, optimizer, loss_fun, train_loader, device, epoch)\n",
    "        epoch_val_loss, val_Y_pred = validate_fun(model, loss_fun, val_loader, device, epoch)\n",
    "\n",
    "        # print(f'Epoch: {epoch}, Train Loss: {epoch_train_loss}, Val Loss: {epoch_val_loss}')\n",
    "\n",
    "        if epoch_val_loss < best_loss:\n",
    "            best_loss = epoch_val_loss\n",
    "            oof[val_idx] = val_Y_pred\n",
    "            torch.save(model.state_dict(), f'simple_fold_{fold + 1}.pth')\n",
    "    \n",
    "        test_dataset = TestMoADataset(X_test)   \n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = SimpleNet(in_size, 2048, 1024, out_size).to(device)\n",
    "    model.load_state_dict(torch.load(f'simple_fold_{fold + 1}.pth'))\n",
    "    model.to(device)\n",
    "\n",
    "    Y_pred = test_fun(model, test_loader, device)\n",
    "\n",
    "    return best_loss, Y_pred, oof\n",
    "\n",
    "\n",
    "def train_tab_net(fold, X_test, X_train, Y_train, X_val, Y_val, train_size, val_idx, out_size):\n",
    "    oof = np.zeros((train_size, out_size))\n",
    "    tabnet_params = dict(\n",
    "                        n_d=32,\n",
    "                        n_a=32,\n",
    "                        n_steps=1,\n",
    "                        gamma=1.8,\n",
    "                        lambda_sparse=0,\n",
    "                        optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=TAB_LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
    "                        scheduler_params=dict(mode='min',\n",
    "                                              patience=5,\n",
    "                                              min_lr=1e-5,\n",
    "                                              factor=0.9),\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                        mask_type='entmax',\n",
    "                        seed=SEED,\n",
    "                        verbose=10\n",
    "                    )\n",
    "\n",
    "    model = TabNetRegressor(**tabnet_params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train=X_train, \n",
    "        y_train=Y_train,\n",
    "        eval_set = [(X_val, Y_val)],\n",
    "        eval_name=['val'],\n",
    "        eval_metric=[LogitsLogLoss],\n",
    "        max_epochs=TAB_EPOCHS,\n",
    "        patience=20,\n",
    "        batch_size=TAB_BATCH_SIZE,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=1,\n",
    "        drop_last=False,\n",
    "        loss_fn=nn.BCEWithLogitsLoss()\n",
    "    )\n",
    "\n",
    "    oof[val_idx] = expit(model.predict(X_val))\n",
    "    Y_pred = expit(model.predict(X_test))\n",
    "\n",
    "    return np.min(model.history['val_logits_ll']), Y_pred, oof\n",
    "\n",
    "\n",
    "def run_msk_fold_cv(X_train, Y_train, Y_train_stub, X_test, ss, num_folds, model_name, device):\n",
    "    running_loss = 0\n",
    "    Y_pred = np.zeros((X_test.shape[0], Y_train.shape[1] - 1))\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
    "    oof = np.zeros((X_train.shape[0], Y_train.shape[1] - 1))\n",
    "\n",
    "    for fold, (trn_idx, val_idx) in enumerate(mskf.split(X_train, Y_train)):\n",
    "        fold_X_train = X_train.loc[trn_idx, :]\n",
    "        fold_Y_train = Y_train.loc[trn_idx, :].drop('sig_id', axis=1)\n",
    "        fold_X_val = X_train.loc[val_idx, :]\n",
    "        fold_Y_val = Y_train.loc[val_idx, :].drop('sig_id', axis=1)\n",
    "\n",
    "        train_dataset = MoADataset(fold_X_train, fold_Y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        val_dataset = MoADataset(fold_X_val, fold_Y_val)   \n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        print(f'Fold: {fold + 1}')\n",
    "\n",
    "        if model_name == 'simple':\n",
    "            fold_loss, fold_Y_pred, fold_oof = train_simple_net(fold, X_test.drop('sig_id', axis=1), \n",
    "                                                                train_loader, val_loader, \n",
    "                                                                X_train.shape[0], val_idx, \n",
    "                                                                X_train.shape[1], Y_train.shape[1] - 1, \n",
    "                                                                device)\n",
    "        elif model_name == 'tab':\n",
    "            fold_X_train = fold_X_train.to_numpy()\n",
    "            fold_Y_train = fold_Y_train.to_numpy()\n",
    "            fold_X_val = fold_X_val.to_numpy()\n",
    "            fold_Y_val = fold_Y_val.to_numpy()\n",
    "            fold_loss, fold_Y_pred, fold_oof = train_tab_net(fold, X_test.drop('sig_id', axis=1).to_numpy(), \n",
    "                                                             fold_X_train, fold_Y_train,\n",
    "                                                             fold_X_val, fold_Y_val,\n",
    "                                                             X_train.shape[0], val_idx, \n",
    "                                                             Y_train.shape[1] - 1)\n",
    "        Y_pred += fold_Y_pred\n",
    "        oof += fold_oof\n",
    "        running_loss += fold_loss\n",
    "\n",
    "    Y_pred /= num_folds\n",
    "    oof /= num_folds\n",
    "    cv_loss = running_loss / num_folds\n",
    "\n",
    "    oof_Y_pred = Y_train.copy()\n",
    "    oof_Y_pred.iloc[:, 1:] = oof\n",
    "    oof_Y_pred = Y_train_stub.loc[:, ['sig_id']].merge(oof_Y_pred, on='sig_id', how='left').fillna(0)\n",
    "\n",
    "    Y_true = Y_train_stub.iloc[:, 1:].values\n",
    "    oof_Y_pred = oof_Y_pred.iloc[:, 1:].values\n",
    "\n",
    "    cv_score = 0\n",
    "\n",
    "    for i in range(oof_Y_pred.shape[1]):\n",
    "        cv_score += log_loss(Y_true[:, i], oof_Y_pred[:, i])\n",
    "\n",
    "    cv_score /= oof_Y_pred.shape[1]\n",
    "\n",
    "    print(f'CV loss (ctl_vechile excluded): {cv_loss:.6f}')\n",
    "    print(f'CV loss: {cv_score:.6f}')\n",
    "\n",
    "    test_Y_pred = X_test.loc[:, ['sig_id']].merge(ss, how='left', on=['sig_id'])\n",
    "    test_Y_pred.iloc[:, 1:] = Y_pred\n",
    "    test_Y_pred = ss.loc[:, ['sig_id']].merge(test_Y_pred, on='sig_id', how='left').fillna(0)\n",
    "\n",
    "    return test_Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba63a7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T12:21:29.537082Z",
     "iopub.status.busy": "2021-08-01T12:21:29.536589Z",
     "iopub.status.idle": "2021-08-01T12:21:29.540366Z",
     "shell.execute_reply": "2021-08-01T12:21:29.539946Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.480731Z"
    },
    "papermill": {
     "duration": 0.023605,
     "end_time": "2021-08-01T12:21:29.540490",
     "exception": false,
     "start_time": "2021-08-01T12:21:29.516885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_net(model_name, mode):\n",
    "    use_cuda = False\n",
    "    device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    Y_pred = None\n",
    "\n",
    "    if device == ('cuda'):\n",
    "        use_cuda = True\n",
    "\n",
    "    seed_everything(SEED, use_cuda)\n",
    "\n",
    "    X_train, Y_train, Y_train_stub, X_test, ss = prepare_data('../input/lish-moa')\n",
    "\n",
    "    if mode == 'cv':\n",
    "        Y_pred = run_msk_fold_cv(X_train, Y_train, Y_train_stub, X_test, ss, NUM_FOLDS, model_name, device)\n",
    "  \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed67bebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T12:21:29.617352Z",
     "iopub.status.busy": "2021-08-01T12:21:29.616622Z",
     "iopub.status.idle": "2021-08-01T12:31:08.059137Z",
     "shell.execute_reply": "2021-08-01T12:31:08.058663Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.495788Z"
    },
    "papermill": {
     "duration": 578.505827,
     "end_time": "2021-08-01T12:31:08.059276",
     "exception": false,
     "start_time": "2021-08-01T12:21:29.553449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.3576  | val_logits_ll: 0.04619 |  0:00:02s\n",
      "epoch 10 | loss: 0.01994 | val_logits_ll: 0.01992 |  0:00:13s\n",
      "epoch 20 | loss: 0.01767 | val_logits_ll: 0.01782 |  0:00:24s\n",
      "epoch 30 | loss: 0.01677 | val_logits_ll: 0.0175  |  0:00:35s\n",
      "epoch 40 | loss: 0.01636 | val_logits_ll: 0.01804 |  0:00:46s\n",
      "epoch 50 | loss: 0.01597 | val_logits_ll: 0.01728 |  0:00:58s\n",
      "epoch 60 | loss: 0.01586 | val_logits_ll: 0.01698 |  0:01:10s\n",
      "epoch 70 | loss: 0.01549 | val_logits_ll: 0.017   |  0:01:21s\n",
      "epoch 80 | loss: 0.01501 | val_logits_ll: 0.01747 |  0:01:33s\n",
      "\n",
      "Early stopping occurred at epoch 82 with best_epoch = 62 and best_val_logits_ll = 0.01683\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36189 | val_logits_ll: 0.04236 |  0:00:01s\n",
      "epoch 10 | loss: 0.01957 | val_logits_ll: 0.01972 |  0:00:12s\n",
      "epoch 20 | loss: 0.0175  | val_logits_ll: 0.0192  |  0:00:24s\n",
      "epoch 30 | loss: 0.01698 | val_logits_ll: 0.01949 |  0:00:35s\n",
      "epoch 40 | loss: 0.01618 | val_logits_ll: 0.01739 |  0:00:47s\n",
      "epoch 50 | loss: 0.01598 | val_logits_ll: 0.01856 |  0:00:58s\n",
      "epoch 60 | loss: 0.01564 | val_logits_ll: 0.01882 |  0:01:09s\n",
      "epoch 70 | loss: 0.01529 | val_logits_ll: 0.01731 |  0:01:21s\n",
      "epoch 80 | loss: 0.01509 | val_logits_ll: 0.01779 |  0:01:33s\n",
      "epoch 90 | loss: 0.01455 | val_logits_ll: 0.01733 |  0:01:44s\n",
      "\n",
      "Early stopping occurred at epoch 98 with best_epoch = 78 and best_val_logits_ll = 0.01718\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.35974 | val_logits_ll: 0.0414  |  0:00:01s\n",
      "epoch 10 | loss: 0.01973 | val_logits_ll: 0.01989 |  0:00:13s\n",
      "epoch 20 | loss: 0.01727 | val_logits_ll: 0.01809 |  0:00:24s\n",
      "epoch 30 | loss: 0.01671 | val_logits_ll: 0.02058 |  0:00:35s\n",
      "epoch 40 | loss: 0.01612 | val_logits_ll: 0.01892 |  0:00:46s\n",
      "epoch 50 | loss: 0.01614 | val_logits_ll: 0.01787 |  0:00:57s\n",
      "epoch 60 | loss: 0.01567 | val_logits_ll: 0.01726 |  0:01:08s\n",
      "epoch 70 | loss: 0.0154  | val_logits_ll: 0.01714 |  0:01:21s\n",
      "epoch 80 | loss: 0.01491 | val_logits_ll: 0.01714 |  0:01:32s\n",
      "epoch 90 | loss: 0.01489 | val_logits_ll: 0.01732 |  0:01:43s\n",
      "\n",
      "Early stopping occurred at epoch 95 with best_epoch = 75 and best_val_logits_ll = 0.017\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36608 | val_logits_ll: 0.04519 |  0:00:01s\n",
      "epoch 10 | loss: 0.01959 | val_logits_ll: 0.01968 |  0:00:12s\n",
      "epoch 20 | loss: 0.01756 | val_logits_ll: 0.01804 |  0:00:23s\n",
      "epoch 30 | loss: 0.01687 | val_logits_ll: 0.0177  |  0:00:35s\n",
      "epoch 40 | loss: 0.01622 | val_logits_ll: 0.01929 |  0:00:46s\n",
      "epoch 50 | loss: 0.01599 | val_logits_ll: 0.01712 |  0:00:57s\n",
      "epoch 60 | loss: 0.01603 | val_logits_ll: 0.01738 |  0:01:09s\n",
      "epoch 70 | loss: 0.0158  | val_logits_ll: 0.01695 |  0:01:19s\n",
      "epoch 80 | loss: 0.01571 | val_logits_ll: 0.01703 |  0:01:31s\n",
      "epoch 90 | loss: 0.01556 | val_logits_ll: 0.01714 |  0:01:43s\n",
      "epoch 100| loss: 0.01538 | val_logits_ll: 0.01717 |  0:01:54s\n",
      "epoch 110| loss: 0.01528 | val_logits_ll: 0.01712 |  0:02:05s\n",
      "\n",
      "Early stopping occurred at epoch 112 with best_epoch = 92 and best_val_logits_ll = 0.01687\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.3659  | val_logits_ll: 0.04331 |  0:00:01s\n",
      "epoch 10 | loss: 0.01982 | val_logits_ll: 0.0199  |  0:00:12s\n",
      "epoch 20 | loss: 0.01751 | val_logits_ll: 0.01871 |  0:00:23s\n",
      "epoch 30 | loss: 0.01664 | val_logits_ll: 0.01736 |  0:00:34s\n",
      "epoch 40 | loss: 0.01639 | val_logits_ll: 0.01738 |  0:00:46s\n",
      "epoch 50 | loss: 0.01632 | val_logits_ll: 0.01749 |  0:00:58s\n",
      "epoch 60 | loss: 0.01569 | val_logits_ll: 0.01721 |  0:01:09s\n",
      "epoch 70 | loss: 0.01546 | val_logits_ll: 0.0171  |  0:01:21s\n",
      "epoch 80 | loss: 0.01544 | val_logits_ll: 0.01781 |  0:01:32s\n",
      "epoch 90 | loss: 0.0149  | val_logits_ll: 0.01717 |  0:01:43s\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 74 and best_val_logits_ll = 0.01696\n",
      "Best weights from best epoch are automatically used!\n",
      "CV loss (ctl_vechile excluded): 0.016967\n",
      "CV loss: 0.018854\n"
     ]
    }
   ],
   "source": [
    "sub = run_net('tab', 'cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc200c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T12:31:08.123219Z",
     "iopub.status.busy": "2021-08-01T12:31:08.122646Z",
     "iopub.status.idle": "2021-08-01T12:31:09.601629Z",
     "shell.execute_reply": "2021-08-01T12:31:09.600684Z",
     "shell.execute_reply.started": "2021-08-01T05:50:38.623201Z"
    },
    "papermill": {
     "duration": 1.512706,
     "end_time": "2021-08-01T12:31:09.601779",
     "exception": false,
     "start_time": "2021-08-01T12:31:08.089073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 625.770406,
   "end_time": "2021-08-01T12:31:11.156937",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-01T12:20:45.386531",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
