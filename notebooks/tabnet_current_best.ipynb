{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0e602a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-31T05:08:02.866997Z",
     "iopub.status.busy": "2021-07-31T05:08:02.866446Z",
     "iopub.status.idle": "2021-07-31T05:08:38.092942Z",
     "shell.execute_reply": "2021-07-31T05:08:38.092202Z",
     "shell.execute_reply.started": "2021-07-31T04:59:03.295760Z"
    },
    "papermill": {
     "duration": 35.24809,
     "end_time": "2021-07-31T05:08:38.093114",
     "exception": false,
     "start_time": "2021-07-31T05:08:02.845024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.7.0)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.19.5)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.6.3)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (0.23.2)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (4.61.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.0.1)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.18.2)\r\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (3.7.4.3)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.6)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-3.1.1\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Processing /kaggle/input/iterative-stratification/iterative-stratification-master\r\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\r\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.19.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.6.3)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (0.23.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (1.0.1)\r\n",
      "Building wheels for collected packages: iterative-stratification\r\n",
      "  Building wheel for iterative-stratification (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for iterative-stratification: filename=iterative_stratification-0.1.6-py3-none-any.whl size=8401 sha256=fbe4da5fb960459ae72b0349355df68ffe2b6551938f243b7e175a732391f0c9\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/47/3f/eb4af42d124f37d23d6f13a4c8bbc32c1d70140e6e1cecb4aa\r\n",
      "Successfully built iterative-stratification\r\n",
      "Installing collected packages: iterative-stratification\r\n",
      "Successfully installed iterative-stratification-0.1.6\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl pytorch-tabnet\n",
    "!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6878c72a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T05:08:38.126952Z",
     "iopub.status.busy": "2021-07-31T05:08:38.126416Z",
     "iopub.status.idle": "2021-07-31T05:08:38.150571Z",
     "shell.execute_reply": "2021-07-31T05:08:38.149941Z",
     "shell.execute_reply.started": "2021-07-31T04:59:38.112879Z"
    },
    "papermill": {
     "duration": 0.042565,
     "end_time": "2021-07-31T05:08:38.150731",
     "exception": false,
     "start_time": "2021-07-31T05:08:38.108166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/lish-moa/train_targets_scored.csv\n",
      "/kaggle/input/lish-moa/sample_submission.csv\n",
      "/kaggle/input/lish-moa/train_drug.csv\n",
      "/kaggle/input/lish-moa/train_targets_nonscored.csv\n",
      "/kaggle/input/lish-moa/train_features.csv\n",
      "/kaggle/input/lish-moa/test_features.csv\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.0-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.1-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-1.2.0-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-3.0.0-py3-none-any.whl\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/.travis.yml\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/setup.cfg\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/LICENSE\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/.gitignore\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/README.md\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/setup.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/tests/test_ml_stratifiers.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/tests/__init__.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/iterstrat/ml_stratifiers.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/iterstrat/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ded8692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T05:08:38.180065Z",
     "iopub.status.busy": "2021-07-31T05:08:38.179222Z",
     "iopub.status.idle": "2021-07-31T05:08:40.326547Z",
     "shell.execute_reply": "2021-07-31T05:08:40.326084Z",
     "shell.execute_reply.started": "2021-07-31T04:59:38.134260Z"
    },
    "papermill": {
     "duration": 2.163178,
     "end_time": "2021-07-31T05:08:40.326675",
     "exception": false,
     "start_time": "2021-07-31T05:08:38.163497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import log_loss\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27efbc1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T05:08:40.359858Z",
     "iopub.status.busy": "2021-07-31T05:08:40.359160Z",
     "iopub.status.idle": "2021-07-31T05:08:40.362894Z",
     "shell.execute_reply": "2021-07-31T05:08:40.363320Z",
     "shell.execute_reply.started": "2021-07-31T04:59:38.143591Z"
    },
    "papermill": {
     "duration": 0.02369,
     "end_time": "2021-07-31T05:08:40.363465",
     "exception": false,
     "start_time": "2021-07-31T05:08:40.339775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogitsLogLoss(Metric):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = 'logits_ll'\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        aux = (1 - y_true) * np.log(1 - expit(y_pred) + 1e-15) + y_true * np.log(expit(y_pred) + 1e-15)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eafd5ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T05:08:40.393500Z",
     "iopub.status.busy": "2021-07-31T05:08:40.392997Z",
     "iopub.status.idle": "2021-07-31T05:08:40.396734Z",
     "shell.execute_reply": "2021-07-31T05:08:40.396310Z",
     "shell.execute_reply.started": "2021-07-31T04:59:38.153024Z"
    },
    "papermill": {
     "duration": 0.019874,
     "end_time": "2021-07-31T05:08:40.396844",
     "exception": false,
     "start_time": "2021-07-31T05:08:40.376970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed, use_cuda=False):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2410da27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T05:08:40.428882Z",
     "iopub.status.busy": "2021-07-31T05:08:40.428280Z",
     "iopub.status.idle": "2021-07-31T05:08:40.431472Z",
     "shell.execute_reply": "2021-07-31T05:08:40.431871Z",
     "shell.execute_reply.started": "2021-07-31T04:59:38.161347Z"
    },
    "papermill": {
     "duration": 0.022438,
     "end_time": "2021-07-31T05:08:40.432070",
     "exception": false,
     "start_time": "2021-07-31T05:08:40.409632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    df['cp_time'] = df['cp_time'].map({24: 0, 48: 1, 72:2})\n",
    "    df = df[df['cp_type']  != 'ctl_vehicle'].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_data(dir):\n",
    "    X_train = pd.read_csv(f'{dir}/train_features.csv')\n",
    "    Y_train = pd.read_csv(f'{dir}/train_targets_scored.csv')\n",
    "    X_test = pd.read_csv(f'{dir}/test_features.csv')\n",
    "    ss = pd.read_csv(f'{dir}/sample_submission.csv')\n",
    "\n",
    "    train = X_train.merge(Y_train, on='sig_id')\n",
    "    Y_train_stub = train.loc[:, Y_train.columns]\n",
    "    \n",
    "    train = preprocess(train)\n",
    "    X_test = preprocess(X_test).drop(['cp_type'], axis=1)\n",
    "\n",
    "    X_train = train.loc[:, X_train.columns].drop(['sig_id', 'cp_type'], axis=1)\n",
    "    Y_train = train.loc[:, Y_train.columns]\n",
    "\n",
    "    return X_train, Y_train, Y_train_stub, X_test, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054e78b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T05:08:40.462467Z",
     "iopub.status.busy": "2021-07-31T05:08:40.461896Z",
     "iopub.status.idle": "2021-07-31T05:08:40.466093Z",
     "shell.execute_reply": "2021-07-31T05:08:40.465615Z",
     "shell.execute_reply.started": "2021-07-31T04:59:38.174785Z"
    },
    "papermill": {
     "duration": 0.021873,
     "end_time": "2021-07-31T05:08:40.466214",
     "exception": false,
     "start_time": "2021-07-31T05:08:40.444341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 2e-2\n",
    "WEIGHT_DECAY = 2e-5\n",
    "SEED = 42\n",
    "NUM_FOLDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aae4abfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T05:08:40.607271Z",
     "iopub.status.busy": "2021-07-31T05:08:40.494717Z",
     "iopub.status.idle": "2021-07-31T05:08:40.609640Z",
     "shell.execute_reply": "2021-07-31T05:08:40.609208Z",
     "shell.execute_reply.started": "2021-07-31T04:59:38.183842Z"
    },
    "papermill": {
     "duration": 0.129592,
     "end_time": "2021-07-31T05:08:40.609780",
     "exception": false,
     "start_time": "2021-07-31T05:08:40.480188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_tab_net(fold, X_test, X_train, Y_train, X_val, Y_val, train_size, val_idx, out_size):\n",
    "    oof = np.zeros((train_size, out_size))\n",
    "    tabnet_params = dict(\n",
    "                        n_d=32,\n",
    "                        n_a=32,\n",
    "                        n_steps=1,\n",
    "                        gamma=1.7,\n",
    "                        lambda_sparse=0,\n",
    "                        optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
    "                        scheduler_params=dict(mode='min',\n",
    "                                              patience=5,\n",
    "                                              min_lr=1e-5,\n",
    "                                              factor=0.9),\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                        mask_type='entmax',\n",
    "                        seed=SEED,\n",
    "                        verbose=10\n",
    "                    )\n",
    "\n",
    "    model = TabNetRegressor(**tabnet_params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train=X_train, \n",
    "        y_train=Y_train,\n",
    "        eval_set = [(X_val, Y_val)],\n",
    "        eval_name=['val'],\n",
    "        eval_metric=[LogitsLogLoss],\n",
    "        max_epochs=EPOCHS,\n",
    "        patience=20,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=1,\n",
    "        drop_last=False,\n",
    "        loss_fn=nn.BCEWithLogitsLoss()\n",
    "    )\n",
    "\n",
    "    oof[val_idx] = expit(model.predict(X_val))\n",
    "    Y_pred = expit(model.predict(X_test))\n",
    "\n",
    "    return np.min(model.history['val_logits_ll']), Y_pred, oof\n",
    "\n",
    "def run_msk_fold_cv(X_train, Y_train, Y_train_stub, X_test, ss, num_folds, device):\n",
    "    running_loss = 0\n",
    "    Y_pred = np.zeros((X_test.shape[0], Y_train.shape[1] - 1))\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
    "    oof = np.zeros((X_train.shape[0], Y_train.shape[1] - 1))\n",
    "\n",
    "    for fold, (trn_idx, val_idx) in enumerate(mskf.split(X_train, Y_train)):\n",
    "        fold_X_train = X_train.loc[trn_idx, :].to_numpy()\n",
    "        fold_Y_train = Y_train.loc[trn_idx, :].drop('sig_id', axis=1).to_numpy()\n",
    "        fold_X_val = X_train.loc[val_idx, :].to_numpy()\n",
    "        fold_Y_val = Y_train.loc[val_idx, :].drop('sig_id', axis=1).to_numpy()\n",
    "        \n",
    "        print(f'Fold: {fold + 1}')\n",
    "\n",
    "        fold_loss, fold_Y_pred, fold_oof = train_tab_net(fold, X_test.drop('sig_id', axis=1).to_numpy(), \n",
    "                                                         fold_X_train, fold_Y_train,\n",
    "                                                         fold_X_val, fold_Y_val,\n",
    "                                                         X_train.shape[0], val_idx, \n",
    "                                                         Y_train.shape[1] - 1)\n",
    "        \n",
    "        Y_pred += fold_Y_pred\n",
    "        oof += fold_oof\n",
    "        running_loss += fold_loss\n",
    "        \n",
    "    Y_pred /= num_folds\n",
    "    oof /= num_folds\n",
    "    cv_loss = running_loss / num_folds\n",
    "\n",
    "    oof_Y_pred = Y_train.copy()\n",
    "    oof_Y_pred.iloc[:, 1:] = oof\n",
    "    oof_Y_pred = Y_train_stub.loc[:, ['sig_id']].merge(oof_Y_pred, on='sig_id', how='left').fillna(0)\n",
    "\n",
    "    Y_true = Y_train_stub.iloc[:, 1:].values\n",
    "    oof_Y_pred = oof_Y_pred.iloc[:, 1:].values\n",
    "\n",
    "    cv_score = 0\n",
    "\n",
    "    for i in range(oof_Y_pred.shape[1]):\n",
    "        cv_score += log_loss(Y_true[:, i], oof_Y_pred[:, i])\n",
    "\n",
    "    cv_score /= oof_Y_pred.shape[1]\n",
    "\n",
    "    print(f'CV loss (ctl_vechile excluded): {cv_loss:.6f}')\n",
    "    print(f'CV loss: {cv_score:.6f}')\n",
    "\n",
    "    test_Y_pred = X_test.loc[:, ['sig_id']].merge(ss, how='left', on=['sig_id'])\n",
    "    test_Y_pred.iloc[:, 1:] = Y_pred\n",
    "    test_Y_pred = ss.loc[:, ['sig_id']].merge(test_Y_pred, on='sig_id', how='left').fillna(0)\n",
    "\n",
    "    return test_Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4bb5223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T05:08:40.639596Z",
     "iopub.status.busy": "2021-07-31T05:08:40.639102Z",
     "iopub.status.idle": "2021-07-31T05:08:40.642454Z",
     "shell.execute_reply": "2021-07-31T05:08:40.642866Z",
     "shell.execute_reply.started": "2021-07-31T04:59:38.205651Z"
    },
    "papermill": {
     "duration": 0.020123,
     "end_time": "2021-07-31T05:08:40.642996",
     "exception": false,
     "start_time": "2021-07-31T05:08:40.622873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_net():\n",
    "    use_cuda = False\n",
    "    device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    Y_pred = None\n",
    "\n",
    "    if device == ('cuda'):\n",
    "        use_cuda = True\n",
    "\n",
    "    seed_everything(SEED, use_cuda)\n",
    "\n",
    "    X_train, Y_train, Y_train_stub, X_test, ss = prepare_data('../input/lish-moa')\n",
    "\n",
    "    Y_pred = run_msk_fold_cv(X_train, Y_train, Y_train_stub, X_test, ss, NUM_FOLDS, device)\n",
    "\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6152cc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T05:08:40.714326Z",
     "iopub.status.busy": "2021-07-31T05:08:40.713633Z",
     "iopub.status.idle": "2021-07-31T05:37:06.140088Z",
     "shell.execute_reply": "2021-07-31T05:37:06.139555Z",
     "shell.execute_reply.started": "2021-07-31T04:59:38.217927Z"
    },
    "papermill": {
     "duration": 1705.484523,
     "end_time": "2021-07-31T05:37:06.140236",
     "exception": false,
     "start_time": "2021-07-31T05:08:40.655713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32664 | val_logits_ll: 0.03469 |  0:00:02s\n",
      "epoch 10 | loss: 0.01915 | val_logits_ll: 0.0212  |  0:00:13s\n",
      "epoch 20 | loss: 0.01778 | val_logits_ll: 0.0201  |  0:00:25s\n",
      "epoch 30 | loss: 0.01731 | val_logits_ll: 0.01973 |  0:00:37s\n",
      "epoch 40 | loss: 0.01709 | val_logits_ll: 0.01767 |  0:00:49s\n",
      "epoch 50 | loss: 0.01677 | val_logits_ll: 0.01868 |  0:01:01s\n",
      "epoch 60 | loss: 0.0168  | val_logits_ll: 0.01724 |  0:01:13s\n",
      "epoch 70 | loss: 0.01658 | val_logits_ll: 0.01718 |  0:01:24s\n",
      "epoch 80 | loss: 0.01643 | val_logits_ll: 0.01703 |  0:01:36s\n",
      "epoch 90 | loss: 0.01628 | val_logits_ll: 0.01695 |  0:01:48s\n",
      "epoch 100| loss: 0.01624 | val_logits_ll: 0.01694 |  0:02:01s\n",
      "epoch 110| loss: 0.01602 | val_logits_ll: 0.01686 |  0:02:13s\n",
      "epoch 120| loss: 0.01593 | val_logits_ll: 0.01694 |  0:02:25s\n",
      "epoch 130| loss: 0.01576 | val_logits_ll: 0.017   |  0:02:36s\n",
      "epoch 140| loss: 0.01565 | val_logits_ll: 0.01693 |  0:02:48s\n",
      "\n",
      "Early stopping occurred at epoch 148 with best_epoch = 128 and best_val_logits_ll = 0.01679\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32765 | val_logits_ll: 0.03048 |  0:00:01s\n",
      "epoch 10 | loss: 0.01883 | val_logits_ll: 0.01906 |  0:00:13s\n",
      "epoch 20 | loss: 0.01751 | val_logits_ll: 0.01864 |  0:00:25s\n",
      "epoch 30 | loss: 0.0173  | val_logits_ll: 0.01776 |  0:00:37s\n",
      "epoch 40 | loss: 0.01701 | val_logits_ll: 0.01761 |  0:00:49s\n",
      "epoch 50 | loss: 0.01673 | val_logits_ll: 0.01809 |  0:01:00s\n",
      "epoch 60 | loss: 0.0167  | val_logits_ll: 0.01871 |  0:01:13s\n",
      "epoch 70 | loss: 0.01653 | val_logits_ll: 0.01712 |  0:01:25s\n",
      "epoch 80 | loss: 0.01645 | val_logits_ll: 0.01699 |  0:01:36s\n",
      "epoch 90 | loss: 0.01639 | val_logits_ll: 0.01724 |  0:01:49s\n",
      "epoch 100| loss: 0.01617 | val_logits_ll: 0.017   |  0:02:01s\n",
      "epoch 110| loss: 0.01598 | val_logits_ll: 0.01727 |  0:02:12s\n",
      "epoch 120| loss: 0.0159  | val_logits_ll: 0.01703 |  0:02:24s\n",
      "epoch 130| loss: 0.01568 | val_logits_ll: 0.01703 |  0:02:36s\n",
      "epoch 140| loss: 0.01551 | val_logits_ll: 0.01761 |  0:02:49s\n",
      "\n",
      "Early stopping occurred at epoch 145 with best_epoch = 125 and best_val_logits_ll = 0.01688\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32344 | val_logits_ll: 0.03442 |  0:00:01s\n",
      "epoch 10 | loss: 0.0189  | val_logits_ll: 0.021   |  0:00:12s\n",
      "epoch 20 | loss: 0.01767 | val_logits_ll: 0.01961 |  0:00:24s\n",
      "epoch 30 | loss: 0.01729 | val_logits_ll: 0.01916 |  0:00:36s\n",
      "epoch 40 | loss: 0.01715 | val_logits_ll: 0.01795 |  0:00:48s\n",
      "epoch 50 | loss: 0.01691 | val_logits_ll: 0.01754 |  0:01:00s\n",
      "epoch 60 | loss: 0.01674 | val_logits_ll: 0.01753 |  0:01:12s\n",
      "epoch 70 | loss: 0.01669 | val_logits_ll: 0.01728 |  0:01:24s\n",
      "epoch 80 | loss: 0.0165  | val_logits_ll: 0.01738 |  0:01:36s\n",
      "epoch 90 | loss: 0.01639 | val_logits_ll: 0.01723 |  0:01:47s\n",
      "epoch 100| loss: 0.01631 | val_logits_ll: 0.0175  |  0:02:00s\n",
      "epoch 110| loss: 0.01628 | val_logits_ll: 0.0172  |  0:02:12s\n",
      "epoch 120| loss: 0.0161  | val_logits_ll: 0.01735 |  0:02:23s\n",
      "epoch 130| loss: 0.01586 | val_logits_ll: 0.01711 |  0:02:36s\n",
      "epoch 140| loss: 0.01569 | val_logits_ll: 0.01722 |  0:02:47s\n",
      "epoch 150| loss: 0.01552 | val_logits_ll: 0.01708 |  0:02:59s\n",
      "\n",
      "Early stopping occurred at epoch 152 with best_epoch = 132 and best_val_logits_ll = 0.017\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32593 | val_logits_ll: 0.03143 |  0:00:01s\n",
      "epoch 10 | loss: 0.01894 | val_logits_ll: 0.02103 |  0:00:13s\n",
      "epoch 20 | loss: 0.01755 | val_logits_ll: 0.01828 |  0:00:25s\n",
      "epoch 30 | loss: 0.01726 | val_logits_ll: 0.01764 |  0:00:38s\n",
      "epoch 40 | loss: 0.01706 | val_logits_ll: 0.01793 |  0:00:50s\n",
      "epoch 50 | loss: 0.01695 | val_logits_ll: 0.01732 |  0:01:02s\n",
      "epoch 60 | loss: 0.01683 | val_logits_ll: 0.01756 |  0:01:14s\n",
      "epoch 70 | loss: 0.01654 | val_logits_ll: 0.01758 |  0:01:25s\n",
      "epoch 80 | loss: 0.01646 | val_logits_ll: 0.01758 |  0:01:37s\n",
      "epoch 90 | loss: 0.01649 | val_logits_ll: 0.01707 |  0:01:50s\n",
      "epoch 100| loss: 0.01599 | val_logits_ll: 0.01701 |  0:02:02s\n",
      "epoch 110| loss: 0.01593 | val_logits_ll: 0.01686 |  0:02:13s\n",
      "epoch 120| loss: 0.01577 | val_logits_ll: 0.01674 |  0:02:25s\n",
      "epoch 130| loss: 0.01563 | val_logits_ll: 0.01687 |  0:02:36s\n",
      "epoch 140| loss: 0.01561 | val_logits_ll: 0.01712 |  0:02:49s\n",
      "\n",
      "Early stopping occurred at epoch 142 with best_epoch = 122 and best_val_logits_ll = 0.01671\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32495 | val_logits_ll: 0.03206 |  0:00:01s\n",
      "epoch 10 | loss: 0.01914 | val_logits_ll: 0.02081 |  0:00:12s\n",
      "epoch 20 | loss: 0.01769 | val_logits_ll: 0.01828 |  0:00:24s\n",
      "epoch 30 | loss: 0.01722 | val_logits_ll: 0.01793 |  0:00:36s\n",
      "epoch 40 | loss: 0.0169  | val_logits_ll: 0.01766 |  0:00:48s\n",
      "epoch 50 | loss: 0.01677 | val_logits_ll: 0.01772 |  0:01:00s\n",
      "epoch 60 | loss: 0.01679 | val_logits_ll: 0.01798 |  0:01:12s\n",
      "epoch 70 | loss: 0.01664 | val_logits_ll: 0.01751 |  0:01:24s\n",
      "epoch 80 | loss: 0.01655 | val_logits_ll: 0.01727 |  0:01:36s\n",
      "epoch 90 | loss: 0.01637 | val_logits_ll: 0.01738 |  0:01:48s\n",
      "epoch 100| loss: 0.01626 | val_logits_ll: 0.01734 |  0:02:01s\n",
      "epoch 110| loss: 0.01613 | val_logits_ll: 0.01732 |  0:02:12s\n",
      "epoch 120| loss: 0.0158  | val_logits_ll: 0.01718 |  0:02:24s\n",
      "epoch 130| loss: 0.0157  | val_logits_ll: 0.01703 |  0:02:36s\n",
      "epoch 140| loss: 0.01583 | val_logits_ll: 0.01725 |  0:02:47s\n",
      "epoch 150| loss: 0.01541 | val_logits_ll: 0.01736 |  0:02:59s\n",
      "\n",
      "Early stopping occurred at epoch 155 with best_epoch = 135 and best_val_logits_ll = 0.01688\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 6\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32709 | val_logits_ll: 0.03296 |  0:00:01s\n",
      "epoch 10 | loss: 0.01887 | val_logits_ll: 0.01957 |  0:00:12s\n",
      "epoch 20 | loss: 0.01778 | val_logits_ll: 0.01831 |  0:00:24s\n",
      "epoch 30 | loss: 0.01742 | val_logits_ll: 0.01751 |  0:00:36s\n",
      "epoch 40 | loss: 0.01702 | val_logits_ll: 0.01794 |  0:00:48s\n",
      "epoch 50 | loss: 0.01708 | val_logits_ll: 0.01775 |  0:01:00s\n",
      "epoch 60 | loss: 0.0168  | val_logits_ll: 0.01709 |  0:01:11s\n",
      "epoch 70 | loss: 0.01659 | val_logits_ll: 0.01699 |  0:01:23s\n",
      "epoch 80 | loss: 0.01642 | val_logits_ll: 0.01705 |  0:01:35s\n",
      "epoch 90 | loss: 0.01636 | val_logits_ll: 0.01691 |  0:01:46s\n",
      "epoch 100| loss: 0.01595 | val_logits_ll: 0.01704 |  0:01:58s\n",
      "epoch 110| loss: 0.01592 | val_logits_ll: 0.017   |  0:02:10s\n",
      "epoch 120| loss: 0.01568 | val_logits_ll: 0.01712 |  0:02:21s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_val_logits_ll = 0.01676\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 7\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32388 | val_logits_ll: 0.03316 |  0:00:01s\n",
      "epoch 10 | loss: 0.0187  | val_logits_ll: 0.02026 |  0:00:12s\n",
      "epoch 20 | loss: 0.01764 | val_logits_ll: 0.01954 |  0:00:25s\n",
      "epoch 30 | loss: 0.0172  | val_logits_ll: 0.01738 |  0:00:37s\n",
      "epoch 40 | loss: 0.01698 | val_logits_ll: 0.01777 |  0:00:48s\n",
      "epoch 50 | loss: 0.01683 | val_logits_ll: 0.01795 |  0:01:00s\n",
      "epoch 60 | loss: 0.01666 | val_logits_ll: 0.01711 |  0:01:11s\n",
      "epoch 70 | loss: 0.01665 | val_logits_ll: 0.01721 |  0:01:23s\n",
      "epoch 80 | loss: 0.01642 | val_logits_ll: 0.01707 |  0:01:35s\n",
      "epoch 90 | loss: 0.01634 | val_logits_ll: 0.02119 |  0:01:47s\n",
      "epoch 100| loss: 0.01629 | val_logits_ll: 0.01716 |  0:01:59s\n",
      "epoch 110| loss: 0.01614 | val_logits_ll: 0.0169  |  0:02:10s\n",
      "epoch 120| loss: 0.01603 | val_logits_ll: 0.0167  |  0:02:22s\n",
      "epoch 130| loss: 0.01587 | val_logits_ll: 0.01685 |  0:02:34s\n",
      "epoch 140| loss: 0.01565 | val_logits_ll: 0.01681 |  0:02:46s\n",
      "\n",
      "Early stopping occurred at epoch 140 with best_epoch = 120 and best_val_logits_ll = 0.0167\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 8\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32234 | val_logits_ll: 0.03271 |  0:00:01s\n",
      "epoch 10 | loss: 0.01907 | val_logits_ll: 0.02063 |  0:00:12s\n",
      "epoch 20 | loss: 0.01767 | val_logits_ll: 0.0199  |  0:00:24s\n",
      "epoch 30 | loss: 0.01713 | val_logits_ll: 0.01897 |  0:00:37s\n",
      "epoch 40 | loss: 0.01701 | val_logits_ll: 0.01753 |  0:00:49s\n",
      "epoch 50 | loss: 0.01694 | val_logits_ll: 0.01748 |  0:01:00s\n",
      "epoch 60 | loss: 0.01661 | val_logits_ll: 0.0179  |  0:01:11s\n",
      "epoch 70 | loss: 0.01656 | val_logits_ll: 0.01772 |  0:01:24s\n",
      "epoch 80 | loss: 0.01633 | val_logits_ll: 0.01712 |  0:01:36s\n",
      "epoch 90 | loss: 0.01635 | val_logits_ll: 0.01711 |  0:01:48s\n",
      "epoch 100| loss: 0.01615 | val_logits_ll: 0.01708 |  0:01:59s\n",
      "epoch 110| loss: 0.01591 | val_logits_ll: 0.01723 |  0:02:11s\n",
      "epoch 120| loss: 0.01583 | val_logits_ll: 0.0172  |  0:02:22s\n",
      "epoch 130| loss: 0.0157  | val_logits_ll: 0.0169  |  0:02:34s\n",
      "\n",
      "Early stopping occurred at epoch 135 with best_epoch = 115 and best_val_logits_ll = 0.01687\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 9\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32702 | val_logits_ll: 0.03321 |  0:00:01s\n",
      "epoch 10 | loss: 0.01893 | val_logits_ll: 0.01944 |  0:00:12s\n",
      "epoch 20 | loss: 0.01774 | val_logits_ll: 0.01936 |  0:00:25s\n",
      "epoch 30 | loss: 0.01733 | val_logits_ll: 0.01864 |  0:00:36s\n",
      "epoch 40 | loss: 0.01709 | val_logits_ll: 0.01817 |  0:00:48s\n",
      "epoch 50 | loss: 0.01689 | val_logits_ll: 0.01731 |  0:01:00s\n",
      "epoch 60 | loss: 0.01688 | val_logits_ll: 0.01739 |  0:01:12s\n",
      "epoch 70 | loss: 0.01678 | val_logits_ll: 0.01725 |  0:01:24s\n",
      "epoch 80 | loss: 0.01645 | val_logits_ll: 0.01739 |  0:01:35s\n",
      "epoch 90 | loss: 0.01654 | val_logits_ll: 0.0178  |  0:01:47s\n",
      "epoch 100| loss: 0.01607 | val_logits_ll: 0.01708 |  0:01:59s\n",
      "epoch 110| loss: 0.01594 | val_logits_ll: 0.01702 |  0:02:11s\n",
      "epoch 120| loss: 0.01573 | val_logits_ll: 0.01723 |  0:02:23s\n",
      "\n",
      "Early stopping occurred at epoch 124 with best_epoch = 104 and best_val_logits_ll = 0.01689\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 10\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.31945 | val_logits_ll: 0.03204 |  0:00:01s\n",
      "epoch 10 | loss: 0.0191  | val_logits_ll: 0.02098 |  0:00:12s\n",
      "epoch 20 | loss: 0.01771 | val_logits_ll: 0.02061 |  0:00:24s\n",
      "epoch 30 | loss: 0.01723 | val_logits_ll: 0.01768 |  0:00:36s\n",
      "epoch 40 | loss: 0.01721 | val_logits_ll: 0.01759 |  0:00:48s\n",
      "epoch 50 | loss: 0.01702 | val_logits_ll: 0.01795 |  0:01:00s\n",
      "epoch 60 | loss: 0.01691 | val_logits_ll: 0.01726 |  0:01:13s\n",
      "epoch 70 | loss: 0.01685 | val_logits_ll: 0.01727 |  0:01:25s\n",
      "epoch 80 | loss: 0.01672 | val_logits_ll: 0.01724 |  0:01:37s\n",
      "epoch 90 | loss: 0.01652 | val_logits_ll: 0.01714 |  0:01:49s\n",
      "epoch 100| loss: 0.01645 | val_logits_ll: 0.01691 |  0:02:00s\n",
      "epoch 110| loss: 0.01627 | val_logits_ll: 0.01698 |  0:02:12s\n",
      "epoch 120| loss: 0.01608 | val_logits_ll: 0.01715 |  0:02:25s\n",
      "epoch 130| loss: 0.0159  | val_logits_ll: 0.01711 |  0:02:37s\n",
      "\n",
      "Early stopping occurred at epoch 134 with best_epoch = 114 and best_val_logits_ll = 0.01691\n",
      "Best weights from best epoch are automatically used!\n",
      "CV loss (ctl_vechile excluded): 0.016838\n",
      "CV loss: 0.020747\n"
     ]
    }
   ],
   "source": [
    "sub = run_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90a931c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T05:37:06.253785Z",
     "iopub.status.busy": "2021-07-31T05:37:06.252960Z",
     "iopub.status.idle": "2021-07-31T05:37:07.758007Z",
     "shell.execute_reply": "2021-07-31T05:37:07.757441Z"
    },
    "papermill": {
     "duration": 1.563269,
     "end_time": "2021-07-31T05:37:07.758139",
     "exception": false,
     "start_time": "2021-07-31T05:37:06.194870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1753.186399,
   "end_time": "2021-07-31T05:37:09.523879",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-31T05:07:56.337480",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
