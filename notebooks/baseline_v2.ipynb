{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl pytorch-tabnet\r\n",
    "!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in links: /kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.19.5)\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.7.0)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (0.23.2)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (4.61.1)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.6.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.0.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (2.1.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.18.2)\r\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (3.7.4.3)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.6)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-3.1.1\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Processing /kaggle/input/iterative-stratification/iterative-stratification-master\r\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\r\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.19.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.6.3)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (0.23.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (1.0.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (2.1.0)\r\n",
      "Building wheels for collected packages: iterative-stratification\r\n",
      "  Building wheel for iterative-stratification (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for iterative-stratification: filename=iterative_stratification-0.1.6-py3-none-any.whl size=8401 sha256=6a2a75aa7a0e62c90d57151db202de54e76c0c5ffcb843fa9efa536c281562ec\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/47/3f/eb4af42d124f37d23d6f13a4c8bbc32c1d70140e6e1cecb4aa\r\n",
      "Successfully built iterative-stratification\r\n",
      "Installing collected packages: iterative-stratification\r\n",
      "Successfully installed iterative-stratification-0.1.6\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-08-01T05:53:55.322110Z",
     "iopub.status.busy": "2021-08-01T05:53:55.312476Z",
     "iopub.status.idle": "2021-08-01T05:54:31.392370Z",
     "shell.execute_reply": "2021-08-01T05:54:31.391465Z",
     "shell.execute_reply.started": "2021-08-01T05:40:18.073208Z"
    },
    "papermill": {
     "duration": 36.102157,
     "end_time": "2021-08-01T05:54:31.392515",
     "exception": false,
     "start_time": "2021-08-01T05:53:55.290358",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\r\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\r\n",
    "# For example, here's several helpful packages to load\r\n",
    "\r\n",
    "import numpy as np # linear algebra\r\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
    "\r\n",
    "# Input data files are available in the read-only \"../input/\" directory\r\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\r\n",
    "\r\n",
    "import os\r\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\r\n",
    "    for filename in filenames:\r\n",
    "        print(os.path.join(dirname, filename))\r\n",
    "\r\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \r\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/kaggle/input/iterative-stratification/iterative-stratification-master/.travis.yml\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/setup.cfg\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/LICENSE\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/.gitignore\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/README.md\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/setup.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/tests/test_ml_stratifiers.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/tests/__init__.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/iterstrat/ml_stratifiers.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/iterstrat/__init__.py\n",
      "/kaggle/input/lish-moa/train_targets_scored.csv\n",
      "/kaggle/input/lish-moa/sample_submission.csv\n",
      "/kaggle/input/lish-moa/train_drug.csv\n",
      "/kaggle/input/lish-moa/train_targets_nonscored.csv\n",
      "/kaggle/input/lish-moa/train_features.csv\n",
      "/kaggle/input/lish-moa/test_features.csv\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.0-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.1-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-1.2.0-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnet/pytorch_tabnet-3.0.0-py3-none-any.whl\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:54:31.422973Z",
     "iopub.status.busy": "2021-08-01T05:54:31.422435Z",
     "iopub.status.idle": "2021-08-01T05:54:31.442039Z",
     "shell.execute_reply": "2021-08-01T05:54:31.441586Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.347265Z"
    },
    "papermill": {
     "duration": 0.036484,
     "end_time": "2021-08-01T05:54:31.442152",
     "exception": false,
     "start_time": "2021-08-01T05:54:31.405668",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\r\n",
    "from pytorch_tabnet.metrics import Metric\r\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\r\n",
    "from scipy.special import expit\r\n",
    "from sklearn.metrics import log_loss\r\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:54:31.472332Z",
     "iopub.status.busy": "2021-08-01T05:54:31.471585Z",
     "iopub.status.idle": "2021-08-01T05:54:33.482647Z",
     "shell.execute_reply": "2021-08-01T05:54:33.483200Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.373148Z"
    },
    "papermill": {
     "duration": 2.028364,
     "end_time": "2021-08-01T05:54:33.483346",
     "exception": false,
     "start_time": "2021-08-01T05:54:31.454982",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class MoADataset:\r\n",
    "    def __init__(self, features, targets):\r\n",
    "        self.features = features.values\r\n",
    "        self.targets = targets.values\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return (self.features.shape[0])\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        idx_features = torch.tensor(self.features[idx], dtype=torch.float)\r\n",
    "        idx_targets = torch.tensor(self.targets[idx], dtype=torch.float)\r\n",
    "        return idx_features, idx_targets\r\n",
    "\r\n",
    "\r\n",
    "class TestMoADataset:\r\n",
    "    def __init__(self, features):\r\n",
    "        self.features = features.values\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return (self.features.shape[0])\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        idx_features = torch.tensor(self.features[idx], dtype=torch.float)\r\n",
    "        return idx_features\r\n",
    "    \r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "\r\n",
    "class SimpleNet(nn.Module):\r\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,\r\n",
    "                 output_size, dropout=0.2):\r\n",
    "        super(SimpleNet, self).__init__()\r\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\r\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\r\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.dropout = nn.Dropout(dropout)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.relu(self.fc1(x))\r\n",
    "        x = self.relu(self.fc2(x))\r\n",
    "        x = self.dropout(x)\r\n",
    "        x = self.fc3(x)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:54:33.548677Z",
     "iopub.status.busy": "2021-08-01T05:54:33.547907Z",
     "iopub.status.idle": "2021-08-01T05:54:33.555369Z",
     "shell.execute_reply": "2021-08-01T05:54:33.555882Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.382224Z"
    },
    "papermill": {
     "duration": 0.038586,
     "end_time": "2021-08-01T05:54:33.556060",
     "exception": false,
     "start_time": "2021-08-01T05:54:33.517474",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class LogitsLogLoss(Metric):\r\n",
    "\r\n",
    "    def __init__(self):\r\n",
    "        self._name = 'logits_ll'\r\n",
    "        self._maximize = False\r\n",
    "\r\n",
    "    def __call__(self, y_true, y_pred):\r\n",
    "        aux = (1 - y_true) * np.log(1 - expit(y_pred) + 1e-15) + y_true * np.log(expit(y_pred) + 1e-15)\r\n",
    "        return np.mean(-aux)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:54:33.603752Z",
     "iopub.status.busy": "2021-08-01T05:54:33.601398Z",
     "iopub.status.idle": "2021-08-01T05:54:33.604558Z",
     "shell.execute_reply": "2021-08-01T05:54:33.605372Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.395437Z"
    },
    "papermill": {
     "duration": 0.029007,
     "end_time": "2021-08-01T05:54:33.605542",
     "exception": false,
     "start_time": "2021-08-01T05:54:33.576535",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def seed_everything(seed, use_cuda=False):\r\n",
    "    np.random.seed(seed)\r\n",
    "    torch.manual_seed(seed)\r\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\r\n",
    "\r\n",
    "    if use_cuda:\r\n",
    "        torch.cuda.manual_seed(seed)\r\n",
    "        torch.cuda.manual_seed_all(seed)\r\n",
    "        torch.backends.cudnn.deterministic = True\r\n",
    "        torch.backends.cudnn.benchmark = False"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:54:33.655536Z",
     "iopub.status.busy": "2021-08-01T05:54:33.654611Z",
     "iopub.status.idle": "2021-08-01T05:54:33.657113Z",
     "shell.execute_reply": "2021-08-01T05:54:33.656300Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.411053Z"
    },
    "papermill": {
     "duration": 0.03064,
     "end_time": "2021-08-01T05:54:33.657262",
     "exception": false,
     "start_time": "2021-08-01T05:54:33.626622",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def preprocess(df):\r\n",
    "    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\r\n",
    "    df['cp_time'] = df['cp_time'].map({24: 0, 48: 1, 72:2})\r\n",
    "    df = df[df['cp_type']  != 'ctl_vehicle'].reset_index(drop=True)\r\n",
    "    return df\r\n",
    "\r\n",
    "\r\n",
    "def prepare_data(dir):\r\n",
    "    X_train = pd.read_csv(f'{dir}/train_features.csv')\r\n",
    "    Y_train = pd.read_csv(f'{dir}/train_targets_scored.csv')\r\n",
    "    X_test = pd.read_csv(f'{dir}/test_features.csv')\r\n",
    "    ss = pd.read_csv(f'{dir}/sample_submission.csv')\r\n",
    "\r\n",
    "    train = X_train.merge(Y_train, on='sig_id')\r\n",
    "    Y_train_stub = train.loc[:, Y_train.columns]\r\n",
    "    \r\n",
    "    train = preprocess(train)\r\n",
    "    X_test = preprocess(X_test).drop(['cp_type'], axis=1)\r\n",
    "\r\n",
    "    X_train = train.loc[:, X_train.columns].drop(['sig_id', 'cp_type'], axis=1)\r\n",
    "    Y_train = train.loc[:, Y_train.columns]\r\n",
    "\r\n",
    "    return X_train, Y_train, Y_train_stub, X_test, ss"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:54:33.715913Z",
     "iopub.status.busy": "2021-08-01T05:54:33.715161Z",
     "iopub.status.idle": "2021-08-01T05:54:33.719729Z",
     "shell.execute_reply": "2021-08-01T05:54:33.720645Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.420247Z"
    },
    "papermill": {
     "duration": 0.04085,
     "end_time": "2021-08-01T05:54:33.720823",
     "exception": false,
     "start_time": "2021-08-01T05:54:33.679973",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "TAB_EPOCHS = 200\r\n",
    "TAB_BATCH_SIZE = 1024\r\n",
    "TAB_LEARNING_RATE = 2e-2\r\n",
    "WEIGHT_DECAY = 1e-5\r\n",
    "SEED = 42\r\n",
    "NUM_FOLDS = 5\r\n",
    "\r\n",
    "EPOCHS = 30\r\n",
    "BATCH_SIZE = 2048\r\n",
    "LEARNING_RATE = 1e-3"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:54:33.767422Z",
     "iopub.status.busy": "2021-08-01T05:54:33.766597Z",
     "iopub.status.idle": "2021-08-01T05:54:33.768737Z",
     "shell.execute_reply": "2021-08-01T05:54:33.768124Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.433160Z"
    },
    "papermill": {
     "duration": 0.028237,
     "end_time": "2021-08-01T05:54:33.768898",
     "exception": false,
     "start_time": "2021-08-01T05:54:33.740661",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def train_fun(model, optimizer, loss_fun, train_loader, device, epoch):\r\n",
    "    model.train()\r\n",
    "    running_loss = 0\r\n",
    "\r\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\r\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        outputs = model(inputs)\r\n",
    "        loss = loss_fun(outputs, targets)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        running_loss += loss.item()\r\n",
    "\r\n",
    "        if (batch_idx + 1) % 5 == 0 or (batch_idx + 1) == len(train_loader):\r\n",
    "            print(f'Train Epoch: {epoch + 1}, Batch: [{(batch_idx + 1)}/{len(train_loader)}], Loss: {loss.item():.3f}')\r\n",
    "\r\n",
    "    mean_loss = running_loss / len(train_loader)\r\n",
    "    return mean_loss\r\n",
    "\r\n",
    "\r\n",
    "def validate_fun(model, loss_fun, val_loader, device, epoch):\r\n",
    "    Y_pred_lst = []\r\n",
    "    model.eval()\r\n",
    "    running_loss = 0\r\n",
    "\r\n",
    "    for batch_idx, (inputs, targets) in enumerate(val_loader):\r\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\r\n",
    "        \r\n",
    "        with torch.no_grad():\r\n",
    "            outputs = model(inputs)\r\n",
    "        \r\n",
    "        loss = loss_fun(outputs, targets)\r\n",
    "        running_loss += loss.item()\r\n",
    "        Y_pred_lst.append(outputs.sigmoid().detach().cpu().numpy())\r\n",
    "\r\n",
    "        if (batch_idx + 1) % 1024 == 0 or (batch_idx + 1) == len(val_loader):\r\n",
    "            print(f'Validate Epoch: {epoch + 1}, Batch: [{batch_idx + 1}/{len(val_loader)}], Loss: {loss.item():.6f}')\r\n",
    "\r\n",
    "    mean_loss = running_loss / len(val_loader)\r\n",
    "    Y_pred = np.concatenate(Y_pred_lst)\r\n",
    "\r\n",
    "    return mean_loss, Y_pred\r\n",
    "\r\n",
    "\r\n",
    "def test_fun(model, test_loader, device):\r\n",
    "    Y_pred_lst = []\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    for batch_idx, inputs in enumerate(test_loader):\r\n",
    "        inputs = inputs.to(device)\r\n",
    "        \r\n",
    "        with torch.no_grad():\r\n",
    "            outputs = model(inputs)\r\n",
    "\r\n",
    "        Y_pred_lst.append(outputs.sigmoid().detach().cpu().numpy())\r\n",
    "\r\n",
    "    Y_pred = np.concatenate(Y_pred_lst)\r\n",
    "\r\n",
    "    return Y_pred\r\n",
    "\r\n",
    "\r\n",
    "def train_simple_net(fold, X_test, train_loader, val_loader, train_size, val_idx, in_size, out_size,\r\n",
    "                     device):\r\n",
    "    model = SimpleNet(in_size, 2048, 1024, out_size).to(device)\r\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\r\n",
    "    loss_fun = nn.BCEWithLogitsLoss()\r\n",
    "    best_loss = np.inf\r\n",
    "    oof = np.zeros((train_size, out_size))\r\n",
    "\r\n",
    "    for epoch in range(EPOCHS):\r\n",
    "        epoch_train_loss = train_fun(model, optimizer, loss_fun, train_loader, device, epoch)\r\n",
    "        epoch_val_loss, val_Y_pred = validate_fun(model, loss_fun, val_loader, device, epoch)\r\n",
    "\r\n",
    "        # print(f'Epoch: {epoch}, Train Loss: {epoch_train_loss}, Val Loss: {epoch_val_loss}')\r\n",
    "\r\n",
    "        if epoch_val_loss < best_loss:\r\n",
    "            best_loss = epoch_val_loss\r\n",
    "            oof[val_idx] = val_Y_pred\r\n",
    "            torch.save(model.state_dict(), f'simple_fold_{fold + 1}.pth')\r\n",
    "    \r\n",
    "        test_dataset = TestMoADataset(X_test)   \r\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\r\n",
    "\r\n",
    "    model = SimpleNet(in_size, 2048, 1024, out_size).to(device)\r\n",
    "    model.load_state_dict(torch.load(f'simple_fold_{fold + 1}.pth'))\r\n",
    "    model.to(device)\r\n",
    "\r\n",
    "    Y_pred = test_fun(model, test_loader, device)\r\n",
    "\r\n",
    "    return best_loss, Y_pred, oof\r\n",
    "\r\n",
    "\r\n",
    "def train_tab_net(fold, X_test, X_train, Y_train, X_val, Y_val, train_size, val_idx, out_size):\r\n",
    "    oof = np.zeros((train_size, out_size))\r\n",
    "    tabnet_params = dict(\r\n",
    "                        n_d=32,\r\n",
    "                        n_a=32,\r\n",
    "                        n_steps=1,\r\n",
    "                        gamma=1.8,\r\n",
    "                        lambda_sparse=0,\r\n",
    "                        optimizer_fn=torch.optim.Adam,\r\n",
    "                        optimizer_params=dict(lr=TAB_LEARNING_RATE, weight_decay=WEIGHT_DECAY),\r\n",
    "                        scheduler_params=dict(mode='min',\r\n",
    "                                              patience=5,\r\n",
    "                                              min_lr=1e-5,\r\n",
    "                                              factor=0.9),\r\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\r\n",
    "                        mask_type='entmax',\r\n",
    "                        seed=SEED,\r\n",
    "                        verbose=10\r\n",
    "                    )\r\n",
    "\r\n",
    "    model = TabNetRegressor(**tabnet_params)\r\n",
    "\r\n",
    "    model.fit(\r\n",
    "        X_train=X_train, \r\n",
    "        y_train=Y_train,\r\n",
    "        eval_set = [(X_val, Y_val)],\r\n",
    "        eval_name=['val'],\r\n",
    "        eval_metric=[LogitsLogLoss],\r\n",
    "        max_epochs=TAB_EPOCHS,\r\n",
    "        patience=20,\r\n",
    "        batch_size=TAB_BATCH_SIZE,\r\n",
    "        virtual_batch_size=128,\r\n",
    "        num_workers=1,\r\n",
    "        drop_last=False,\r\n",
    "        loss_fn=nn.BCEWithLogitsLoss()\r\n",
    "    )\r\n",
    "\r\n",
    "    oof[val_idx] = expit(model.predict(X_val))\r\n",
    "    Y_pred = expit(model.predict(X_test))\r\n",
    "\r\n",
    "    return np.min(model.history['val_logits_ll']), Y_pred, oof\r\n",
    "\r\n",
    "\r\n",
    "def run_msk_fold_cv(X_train, Y_train, Y_train_stub, X_test, ss, num_folds, model_name, device):\r\n",
    "    running_loss = 0\r\n",
    "    Y_pred = np.zeros((X_test.shape[0], Y_train.shape[1] - 1))\r\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=num_folds, shuffle=True, random_state=0)\r\n",
    "    oof = np.zeros((X_train.shape[0], Y_train.shape[1] - 1))\r\n",
    "\r\n",
    "    for fold, (trn_idx, val_idx) in enumerate(mskf.split(X_train, Y_train)):\r\n",
    "        fold_X_train = X_train.loc[trn_idx, :]\r\n",
    "        fold_Y_train = Y_train.loc[trn_idx, :].drop('sig_id', axis=1)\r\n",
    "        fold_X_val = X_train.loc[val_idx, :]\r\n",
    "        fold_Y_val = Y_train.loc[val_idx, :].drop('sig_id', axis=1)\r\n",
    "\r\n",
    "        train_dataset = MoADataset(fold_X_train, fold_Y_train)\r\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\r\n",
    "\r\n",
    "        val_dataset = MoADataset(fold_X_val, fold_Y_val)   \r\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\r\n",
    "\r\n",
    "        print(f'Fold: {fold + 1}')\r\n",
    "\r\n",
    "        if model_name == 'simple':\r\n",
    "            fold_loss, fold_Y_pred, fold_oof = train_simple_net(fold, X_test.drop('sig_id', axis=1), \r\n",
    "                                                                train_loader, val_loader, \r\n",
    "                                                                X_train.shape[0], val_idx, \r\n",
    "                                                                X_train.shape[1], Y_train.shape[1] - 1, \r\n",
    "                                                                device)\r\n",
    "        elif model_name == 'tab':\r\n",
    "            fold_X_train = fold_X_train.to_numpy()\r\n",
    "            fold_Y_train = fold_Y_train.to_numpy()\r\n",
    "            fold_X_val = fold_X_val.to_numpy()\r\n",
    "            fold_Y_val = fold_Y_val.to_numpy()\r\n",
    "            fold_loss, fold_Y_pred, fold_oof = train_tab_net(fold, X_test.drop('sig_id', axis=1).to_numpy(), \r\n",
    "                                                             fold_X_train, fold_Y_train,\r\n",
    "                                                             fold_X_val, fold_Y_val,\r\n",
    "                                                             X_train.shape[0], val_idx, \r\n",
    "                                                             Y_train.shape[1] - 1)\r\n",
    "        Y_pred += fold_Y_pred\r\n",
    "        oof += fold_oof\r\n",
    "        running_loss += fold_loss\r\n",
    "\r\n",
    "    Y_pred /= num_folds\r\n",
    "    oof /= num_folds\r\n",
    "    cv_loss = running_loss / num_folds\r\n",
    "\r\n",
    "    oof_Y_pred = Y_train.copy()\r\n",
    "    oof_Y_pred.iloc[:, 1:] = oof\r\n",
    "    oof_Y_pred = Y_train_stub.loc[:, ['sig_id']].merge(oof_Y_pred, on='sig_id', how='left').fillna(0)\r\n",
    "\r\n",
    "    Y_true = Y_train_stub.iloc[:, 1:].values\r\n",
    "    oof_Y_pred = oof_Y_pred.iloc[:, 1:].values\r\n",
    "\r\n",
    "    cv_score = 0\r\n",
    "\r\n",
    "    for i in range(oof_Y_pred.shape[1]):\r\n",
    "        cv_score += log_loss(Y_true[:, i], oof_Y_pred[:, i])\r\n",
    "\r\n",
    "    cv_score /= oof_Y_pred.shape[1]\r\n",
    "\r\n",
    "    print(f'CV loss (ctl_vechile excluded): {cv_loss:.6f}')\r\n",
    "    print(f'CV loss: {cv_score:.6f}')\r\n",
    "\r\n",
    "    test_Y_pred = X_test.loc[:, ['sig_id']].merge(ss, how='left', on=['sig_id'])\r\n",
    "    test_Y_pred.iloc[:, 1:] = Y_pred\r\n",
    "    test_Y_pred = ss.loc[:, ['sig_id']].merge(test_Y_pred, on='sig_id', how='left').fillna(0)\r\n",
    "\r\n",
    "    return test_Y_pred"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:54:33.938987Z",
     "iopub.status.busy": "2021-08-01T05:54:33.937907Z",
     "iopub.status.idle": "2021-08-01T05:54:33.984374Z",
     "shell.execute_reply": "2021-08-01T05:54:33.985378Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.443431Z"
    },
    "papermill": {
     "duration": 0.198985,
     "end_time": "2021-08-01T05:54:33.985580",
     "exception": false,
     "start_time": "2021-08-01T05:54:33.786595",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def run_net(model_name, mode):\r\n",
    "    use_cuda = False\r\n",
    "    device = ('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "    Y_pred = None\r\n",
    "\r\n",
    "    if device == ('cuda'):\r\n",
    "        use_cuda = True\r\n",
    "\r\n",
    "    seed_everything(SEED, use_cuda)\r\n",
    "\r\n",
    "    X_train, Y_train, Y_train_stub, X_test, ss = prepare_data('../input/lish-moa')\r\n",
    "\r\n",
    "    if mode == 'cv':\r\n",
    "        Y_pred = run_msk_fold_cv(X_train, Y_train, Y_train_stub, X_test, ss, NUM_FOLDS, model_name, device)\r\n",
    "  \r\n",
    "    return Y_pred"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:54:34.035422Z",
     "iopub.status.busy": "2021-08-01T05:54:34.034666Z",
     "iopub.status.idle": "2021-08-01T05:54:34.037933Z",
     "shell.execute_reply": "2021-08-01T05:54:34.038501Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.480731Z"
    },
    "papermill": {
     "duration": 0.031512,
     "end_time": "2021-08-01T05:54:34.038681",
     "exception": false,
     "start_time": "2021-08-01T05:54:34.007169",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "sub = run_net('simple', 'cv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fold: 1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.33857 | val_logits_ll: 0.03498 |  0:00:01s\n",
      "epoch 10 | loss: 0.01911 | val_logits_ll: 0.01954 |  0:00:12s\n",
      "epoch 20 | loss: 0.01731 | val_logits_ll: 0.01867 |  0:00:24s\n",
      "epoch 30 | loss: 0.01672 | val_logits_ll: 0.01869 |  0:00:35s\n",
      "epoch 40 | loss: 0.01609 | val_logits_ll: 0.01762 |  0:00:46s\n",
      "epoch 50 | loss: 0.01578 | val_logits_ll: 0.01854 |  0:00:58s\n",
      "epoch 60 | loss: 0.01552 | val_logits_ll: 0.01727 |  0:01:10s\n",
      "epoch 70 | loss: 0.01532 | val_logits_ll: 0.0172  |  0:01:21s\n",
      "epoch 80 | loss: 0.01526 | val_logits_ll: 0.01709 |  0:01:33s\n",
      "\n",
      "Early stopping occurred at epoch 89 with best_epoch = 69 and best_val_logits_ll = 0.0168\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34274 | val_logits_ll: 0.0345  |  0:00:01s\n",
      "epoch 10 | loss: 0.01951 | val_logits_ll: 0.01978 |  0:00:12s\n",
      "epoch 20 | loss: 0.01739 | val_logits_ll: 0.0187  |  0:00:24s\n",
      "epoch 30 | loss: 0.01672 | val_logits_ll: 0.01787 |  0:00:35s\n",
      "epoch 40 | loss: 0.0166  | val_logits_ll: 0.0191  |  0:00:47s\n",
      "epoch 50 | loss: 0.01615 | val_logits_ll: 0.01749 |  0:00:58s\n",
      "epoch 60 | loss: 0.01586 | val_logits_ll: 0.0175  |  0:01:09s\n",
      "epoch 70 | loss: 0.0156  | val_logits_ll: 0.01719 |  0:01:20s\n",
      "epoch 80 | loss: 0.01525 | val_logits_ll: 0.01784 |  0:01:32s\n",
      "epoch 90 | loss: 0.01508 | val_logits_ll: 0.01731 |  0:01:43s\n",
      "epoch 100| loss: 0.01475 | val_logits_ll: 0.01744 |  0:01:55s\n",
      "\n",
      "Early stopping occurred at epoch 103 with best_epoch = 83 and best_val_logits_ll = 0.01706\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34552 | val_logits_ll: 0.03537 |  0:00:01s\n",
      "epoch 10 | loss: 0.01919 | val_logits_ll: 0.02035 |  0:00:11s\n",
      "epoch 20 | loss: 0.01742 | val_logits_ll: 0.01866 |  0:00:22s\n",
      "epoch 30 | loss: 0.01662 | val_logits_ll: 0.02046 |  0:00:34s\n",
      "epoch 40 | loss: 0.01622 | val_logits_ll: 0.01855 |  0:00:45s\n",
      "epoch 50 | loss: 0.01587 | val_logits_ll: 0.01735 |  0:00:56s\n",
      "epoch 60 | loss: 0.01559 | val_logits_ll: 0.01849 |  0:01:08s\n",
      "epoch 70 | loss: 0.01534 | val_logits_ll: 0.01766 |  0:01:18s\n",
      "epoch 80 | loss: 0.01499 | val_logits_ll: 0.01821 |  0:01:29s\n",
      "epoch 90 | loss: 0.0145  | val_logits_ll: 0.0174  |  0:01:41s\n",
      "\n",
      "Early stopping occurred at epoch 91 with best_epoch = 71 and best_val_logits_ll = 0.01705\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34654 | val_logits_ll: 0.03887 |  0:00:01s\n",
      "epoch 10 | loss: 0.01953 | val_logits_ll: 0.02109 |  0:00:12s\n",
      "epoch 20 | loss: 0.01762 | val_logits_ll: 0.019   |  0:00:24s\n",
      "epoch 30 | loss: 0.01683 | val_logits_ll: 0.0191  |  0:00:35s\n",
      "epoch 40 | loss: 0.01607 | val_logits_ll: 0.01738 |  0:00:46s\n",
      "epoch 50 | loss: 0.01569 | val_logits_ll: 0.01804 |  0:00:57s\n",
      "epoch 60 | loss: 0.01555 | val_logits_ll: 0.01821 |  0:01:08s\n",
      "epoch 70 | loss: 0.01517 | val_logits_ll: 0.0187  |  0:01:20s\n",
      "epoch 80 | loss: 0.01518 | val_logits_ll: 0.01712 |  0:01:31s\n",
      "\n",
      "Early stopping occurred at epoch 89 with best_epoch = 69 and best_val_logits_ll = 0.01685\n",
      "Best weights from best epoch are automatically used!\n",
      "Fold: 5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34159 | val_logits_ll: 0.03457 |  0:00:01s\n",
      "epoch 10 | loss: 0.0194  | val_logits_ll: 0.02169 |  0:00:12s\n",
      "epoch 20 | loss: 0.01747 | val_logits_ll: 0.01915 |  0:00:23s\n",
      "epoch 30 | loss: 0.01671 | val_logits_ll: 0.01803 |  0:00:34s\n",
      "epoch 40 | loss: 0.01632 | val_logits_ll: 0.01742 |  0:00:45s\n",
      "epoch 50 | loss: 0.01593 | val_logits_ll: 0.01722 |  0:00:56s\n",
      "epoch 60 | loss: 0.01564 | val_logits_ll: 0.01848 |  0:01:08s\n",
      "epoch 70 | loss: 0.01542 | val_logits_ll: 0.01831 |  0:01:19s\n",
      "epoch 80 | loss: 0.0157  | val_logits_ll: 0.0179  |  0:01:30s\n",
      "\n",
      "Early stopping occurred at epoch 89 with best_epoch = 69 and best_val_logits_ll = 0.01691\n",
      "Best weights from best epoch are automatically used!\n",
      "CV loss (ctl_vechile excluded): 0.016935\n",
      "CV loss: 0.018840\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:54:34.139576Z",
     "iopub.status.busy": "2021-08-01T05:54:34.138743Z",
     "iopub.status.idle": "2021-08-01T06:03:42.993358Z",
     "shell.execute_reply": "2021-08-01T06:03:42.992865Z",
     "shell.execute_reply.started": "2021-08-01T05:40:52.495788Z"
    },
    "papermill": {
     "duration": 548.934153,
     "end_time": "2021-08-01T06:03:42.993492",
     "exception": false,
     "start_time": "2021-08-01T05:54:34.059339",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T06:03:43.054589Z",
     "iopub.status.busy": "2021-08-01T06:03:43.054072Z",
     "iopub.status.idle": "2021-08-01T06:03:44.646722Z",
     "shell.execute_reply": "2021-08-01T06:03:44.647338Z",
     "shell.execute_reply.started": "2021-08-01T05:50:38.623201Z"
    },
    "papermill": {
     "duration": 1.62571,
     "end_time": "2021-08-01T06:03:44.647564",
     "exception": false,
     "start_time": "2021-08-01T06:03:43.021854",
     "status": "completed"
    },
    "tags": []
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 597.8608,
   "end_time": "2021-08-01T06:03:46.797188",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-01T05:53:48.936388",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}